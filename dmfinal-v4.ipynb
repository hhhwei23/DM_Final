{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "639df1da",
   "metadata": {
    "papermill": {
     "duration": 0.003995,
     "end_time": "2024-12-11T13:32:48.619771",
     "exception": false,
     "start_time": "2024-12-11T13:32:48.615776",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# [最高分](https://www.kaggle.com/code/honganzhu/cmi-piu-competition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29a9a731",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T13:32:48.645539Z",
     "iopub.status.busy": "2024-12-11T13:32:48.644813Z",
     "iopub.status.idle": "2024-12-11T13:33:29.810902Z",
     "shell.execute_reply": "2024-12-11T13:33:29.809791Z"
    },
    "papermill": {
     "duration": 41.192741,
     "end_time": "2024-12-11T13:33:29.815997",
     "exception": false,
     "start_time": "2024-12-11T13:32:48.623256",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (1.26.4)\r\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (2.2.3)\r\n",
      "Requirement already satisfied: colorama in /opt/conda/lib/python3.10/site-packages (0.4.6)\r\n",
      "Requirement already satisfied: catboost in /opt/conda/lib/python3.10/site-packages (1.2.7)\r\n",
      "Requirement already satisfied: dask[dataframe] in /opt/conda/lib/python3.10/site-packages (2024.11.2)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas) (2024.1)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas) (2024.1)\r\n",
      "Requirement already satisfied: graphviz in /opt/conda/lib/python3.10/site-packages (from catboost) (0.20.3)\r\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from catboost) (3.7.5)\r\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from catboost) (1.14.1)\r\n",
      "Requirement already satisfied: plotly in /opt/conda/lib/python3.10/site-packages (from catboost) (5.22.0)\r\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from catboost) (1.16.0)\r\n",
      "Requirement already satisfied: click>=8.1 in /opt/conda/lib/python3.10/site-packages (from dask[dataframe]) (8.1.7)\r\n",
      "Requirement already satisfied: cloudpickle>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from dask[dataframe]) (3.1.0)\r\n",
      "Requirement already satisfied: fsspec>=2021.09.0 in /opt/conda/lib/python3.10/site-packages (from dask[dataframe]) (2024.6.0)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from dask[dataframe]) (21.3)\r\n",
      "Requirement already satisfied: partd>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from dask[dataframe]) (1.4.2)\r\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /opt/conda/lib/python3.10/site-packages (from dask[dataframe]) (6.0.2)\r\n",
      "Requirement already satisfied: toolz>=0.10.0 in /opt/conda/lib/python3.10/site-packages (from dask[dataframe]) (0.12.1)\r\n",
      "Requirement already satisfied: importlib-metadata>=4.13.0 in /opt/conda/lib/python3.10/site-packages (from dask[dataframe]) (7.0.0)\r\n",
      "Requirement already satisfied: dask-expr<1.2,>=1.1 in /opt/conda/lib/python3.10/site-packages (from dask[dataframe]) (1.1.19)\r\n",
      "Requirement already satisfied: pyarrow>=14.0.1 in /opt/conda/lib/python3.10/site-packages (from dask-expr<1.2,>=1.1->dask[dataframe]) (17.0.0)\r\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.10/site-packages (from importlib-metadata>=4.13.0->dask[dataframe]) (3.19.2)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->dask[dataframe]) (3.1.2)\r\n",
      "Requirement already satisfied: locket in /opt/conda/lib/python3.10/site-packages (from partd>=1.4.0->dask[dataframe]) (1.0.0)\r\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->catboost) (1.2.1)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib->catboost) (0.12.1)\r\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->catboost) (4.53.0)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->catboost) (1.4.5)\r\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->catboost) (10.3.0)\r\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from plotly->catboost) (8.3.0)\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install numpy pandas colorama catboost dask[dataframe]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30be4ead",
   "metadata": {
    "papermill": {
     "duration": 0.003507,
     "end_time": "2024-12-11T13:33:29.824574",
     "exception": false,
     "start_time": "2024-12-11T13:33:29.821067",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2e84362",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T13:33:29.833700Z",
     "iopub.status.busy": "2024-12-11T13:33:29.833403Z",
     "iopub.status.idle": "2024-12-11T13:33:48.732407Z",
     "shell.execute_reply": "2024-12-11T13:33:48.731637Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 18.906218,
     "end_time": "2024-12-11T13:33:48.734383",
     "exception": false,
     "start_time": "2024-12-11T13:33:29.828165",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from sklearn.base import clone\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.optimize import minimize\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tqdm import tqdm\n",
    "import polars as pl\n",
    "import polars.selectors as cs\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator, FormatStrFormatter, PercentFormatter\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "from keras.optimizers import Adam\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from colorama import Fore, Style\n",
    "from IPython.display import clear_output\n",
    "import warnings\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.ensemble import (\n",
    "    VotingRegressor,\n",
    "    RandomForestRegressor,\n",
    "    GradientBoostingRegressor,\n",
    ")\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.pipeline import Pipeline\n",
    "import copy\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.options.display.max_columns = None\n",
    "\n",
    "\n",
    "class EXEC_ENV_ENUM(Enum):\n",
    "    COLAB = 1\n",
    "    KAGGLE = 2\n",
    "    LOCAL = 3\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "丟到 kaggle 要改的地方\n",
    "\"\"\"\n",
    "# EXEC_ENV = EXEC_ENV_ENUM.COLAB\n",
    "EXEC_ENV = EXEC_ENV_ENUM.KAGGLE\n",
    "# EXEC_ENV = EXEC_ENV_ENUM.LOCAL\n",
    "\n",
    "\"\"\"\n",
    "Constant\n",
    "\"\"\"\n",
    "train_featuresCols = (\n",
    "    [  # 三次 submission train 的 feature columns (第二跟第三次的 column 一樣)\n",
    "        \"Basic_Demos-Age\",\n",
    "        \"Basic_Demos-Sex\",\n",
    "        \"CGAS-CGAS_Score\",\n",
    "        \"Physical-BMI\",\n",
    "        \"Physical-Height\",\n",
    "        \"Physical-Weight\",\n",
    "        \"Physical-Waist_Circumference\",\n",
    "        \"Physical-Diastolic_BP\",\n",
    "        \"Physical-HeartRate\",\n",
    "        \"Physical-Systolic_BP\",\n",
    "        \"Fitness_Endurance-Max_Stage\",\n",
    "        \"Fitness_Endurance-Time_Mins\",\n",
    "        \"Fitness_Endurance-Time_Sec\",\n",
    "        \"FGC-FGC_CU\",\n",
    "        \"FGC-FGC_CU_Zone\",\n",
    "        \"FGC-FGC_GSND\",\n",
    "        \"FGC-FGC_GSND_Zone\",\n",
    "        \"FGC-FGC_GSD\",\n",
    "        \"FGC-FGC_GSD_Zone\",\n",
    "        \"FGC-FGC_PU\",\n",
    "        \"FGC-FGC_PU_Zone\",\n",
    "        \"FGC-FGC_SRL\",\n",
    "        \"FGC-FGC_SRL_Zone\",\n",
    "        \"FGC-FGC_SRR\",\n",
    "        \"FGC-FGC_SRR_Zone\",\n",
    "        \"FGC-FGC_TL\",\n",
    "        \"FGC-FGC_TL_Zone\",\n",
    "        \"BIA-BIA_Activity_Level_num\",\n",
    "        \"BIA-BIA_BMC\",\n",
    "        \"BIA-BIA_BMI\",\n",
    "        \"BIA-BIA_BMR\",\n",
    "        \"BIA-BIA_DEE\",\n",
    "        \"BIA-BIA_ECW\",\n",
    "        \"BIA-BIA_FFM\",\n",
    "        \"BIA-BIA_FFMI\",\n",
    "        \"BIA-BIA_FMI\",\n",
    "        \"BIA-BIA_Fat\",\n",
    "        \"BIA-BIA_Frame_num\",\n",
    "        \"BIA-BIA_ICW\",\n",
    "        \"BIA-BIA_LDM\",\n",
    "        \"BIA-BIA_LST\",\n",
    "        \"BIA-BIA_SMM\",\n",
    "        \"BIA-BIA_TBW\",\n",
    "        \"PAQ_A-PAQ_A_Total\",\n",
    "        \"PAQ_C-PAQ_C_Total\",\n",
    "        \"SDS-SDS_Total_Raw\",\n",
    "        \"SDS-SDS_Total_T\",\n",
    "        \"PreInt_EduHx-computerinternet_hoursday\",\n",
    "        \"sii\",\n",
    "        \"BMI_Age\",\n",
    "        \"Internet_Hours_Age\",\n",
    "        \"BMI_Internet_Hours\",\n",
    "        \"BFP_BMI\",\n",
    "        \"FFMI_BFP\",\n",
    "        \"FMI_BFP\",\n",
    "        \"LST_TBW\",\n",
    "        \"BFP_BMR\",\n",
    "        \"BFP_DEE\",\n",
    "        \"BMR_Weight\",\n",
    "        \"DEE_Weight\",\n",
    "        \"SMM_Height\",\n",
    "        \"Muscle_to_Fat\",\n",
    "        \"Hydration_Status\",\n",
    "        \"ICW_TBW\",\n",
    "    ],\n",
    "    [\n",
    "        \"Basic_Demos-Enroll_Season\",\n",
    "        \"Basic_Demos-Age\",\n",
    "        \"Basic_Demos-Sex\",\n",
    "        \"CGAS-Season\",\n",
    "        \"CGAS-CGAS_Score\",\n",
    "        \"Physical-Season\",\n",
    "        \"Physical-BMI\",\n",
    "        \"Physical-Height\",\n",
    "        \"Physical-Weight\",\n",
    "        \"Physical-Waist_Circumference\",\n",
    "        \"Physical-Diastolic_BP\",\n",
    "        \"Physical-HeartRate\",\n",
    "        \"Physical-Systolic_BP\",\n",
    "        \"Fitness_Endurance-Season\",\n",
    "        \"Fitness_Endurance-Max_Stage\",\n",
    "        \"Fitness_Endurance-Time_Mins\",\n",
    "        \"Fitness_Endurance-Time_Sec\",\n",
    "        \"FGC-Season\",\n",
    "        \"FGC-FGC_CU\",\n",
    "        \"FGC-FGC_CU_Zone\",\n",
    "        \"FGC-FGC_GSND\",\n",
    "        \"FGC-FGC_GSND_Zone\",\n",
    "        \"FGC-FGC_GSD\",\n",
    "        \"FGC-FGC_GSD_Zone\",\n",
    "        \"FGC-FGC_PU\",\n",
    "        \"FGC-FGC_PU_Zone\",\n",
    "        \"FGC-FGC_SRL\",\n",
    "        \"FGC-FGC_SRL_Zone\",\n",
    "        \"FGC-FGC_SRR\",\n",
    "        \"FGC-FGC_SRR_Zone\",\n",
    "        \"FGC-FGC_TL\",\n",
    "        \"FGC-FGC_TL_Zone\",\n",
    "        \"BIA-Season\",\n",
    "        \"BIA-BIA_Activity_Level_num\",\n",
    "        \"BIA-BIA_BMC\",\n",
    "        \"BIA-BIA_BMI\",\n",
    "        \"BIA-BIA_BMR\",\n",
    "        \"BIA-BIA_DEE\",\n",
    "        \"BIA-BIA_ECW\",\n",
    "        \"BIA-BIA_FFM\",\n",
    "        \"BIA-BIA_FFMI\",\n",
    "        \"BIA-BIA_FMI\",\n",
    "        \"BIA-BIA_Fat\",\n",
    "        \"BIA-BIA_Frame_num\",\n",
    "        \"BIA-BIA_ICW\",\n",
    "        \"BIA-BIA_LDM\",\n",
    "        \"BIA-BIA_LST\",\n",
    "        \"BIA-BIA_SMM\",\n",
    "        \"BIA-BIA_TBW\",\n",
    "        \"PAQ_A-Season\",\n",
    "        \"PAQ_A-PAQ_A_Total\",\n",
    "        \"PAQ_C-Season\",\n",
    "        \"PAQ_C-PAQ_C_Total\",\n",
    "        \"SDS-Season\",\n",
    "        \"SDS-SDS_Total_Raw\",\n",
    "        \"SDS-SDS_Total_T\",\n",
    "        \"PreInt_EduHx-Season\",\n",
    "        \"PreInt_EduHx-computerinternet_hoursday\",\n",
    "        \"sii\",\n",
    "    ],\n",
    ")\n",
    "test_featuresCols = [\n",
    "        \"Basic_Demos-Age\",\n",
    "        \"Basic_Demos-Sex\",\n",
    "        \"CGAS-CGAS_Score\",\n",
    "        \"Physical-BMI\",\n",
    "        \"Physical-Height\",\n",
    "        \"Physical-Weight\",\n",
    "        \"Physical-Waist_Circumference\",\n",
    "        \"Physical-Diastolic_BP\",\n",
    "        \"Physical-HeartRate\",\n",
    "        \"Physical-Systolic_BP\",\n",
    "        \"Fitness_Endurance-Max_Stage\",\n",
    "        \"Fitness_Endurance-Time_Mins\",\n",
    "        \"Fitness_Endurance-Time_Sec\",\n",
    "        \"FGC-FGC_CU\",\n",
    "        \"FGC-FGC_CU_Zone\",\n",
    "        \"FGC-FGC_GSND\",\n",
    "        \"FGC-FGC_GSND_Zone\",\n",
    "        \"FGC-FGC_GSD\",\n",
    "        \"FGC-FGC_GSD_Zone\",\n",
    "        \"FGC-FGC_PU\",\n",
    "        \"FGC-FGC_PU_Zone\",\n",
    "        \"FGC-FGC_SRL\",\n",
    "        \"FGC-FGC_SRL_Zone\",\n",
    "        \"FGC-FGC_SRR\",\n",
    "        \"FGC-FGC_SRR_Zone\",\n",
    "        \"FGC-FGC_TL\",\n",
    "        \"FGC-FGC_TL_Zone\",\n",
    "        \"BIA-BIA_Activity_Level_num\",\n",
    "        \"BIA-BIA_BMC\",\n",
    "        \"BIA-BIA_BMI\",\n",
    "        \"BIA-BIA_BMR\",\n",
    "        \"BIA-BIA_DEE\",\n",
    "        \"BIA-BIA_ECW\",\n",
    "        \"BIA-BIA_FFM\",\n",
    "        \"BIA-BIA_FFMI\",\n",
    "        \"BIA-BIA_FMI\",\n",
    "        \"BIA-BIA_Fat\",\n",
    "        \"BIA-BIA_Frame_num\",\n",
    "        \"BIA-BIA_ICW\",\n",
    "        \"BIA-BIA_LDM\",\n",
    "        \"BIA-BIA_LST\",\n",
    "        \"BIA-BIA_SMM\",\n",
    "        \"BIA-BIA_TBW\",\n",
    "        \"PAQ_A-PAQ_A_Total\",\n",
    "        \"PAQ_C-PAQ_C_Total\",\n",
    "        \"SDS-SDS_Total_Raw\",\n",
    "        \"SDS-SDS_Total_T\",\n",
    "        \"PreInt_EduHx-computerinternet_hoursday\",\n",
    "        \"BMI_Age\",\n",
    "        \"Internet_Hours_Age\",\n",
    "        \"BMI_Internet_Hours\",\n",
    "        \"BFP_BMI\",\n",
    "        \"FFMI_BFP\",\n",
    "        \"FMI_BFP\",\n",
    "        \"LST_TBW\",\n",
    "        \"BFP_BMR\",\n",
    "        \"BFP_DEE\",\n",
    "        \"BMR_Weight\",\n",
    "        \"DEE_Weight\",\n",
    "        \"SMM_Height\",\n",
    "        \"Muscle_to_Fat\",\n",
    "        \"Hydration_Status\",\n",
    "        \"ICW_TBW\",\n",
    "    ]\n",
    "season_cols = ['Basic_Demos-Enroll_Season', 'CGAS-Season', 'Physical-Season',\n",
    "          'Fitness_Endurance-Season', 'FGC-Season', 'BIA-Season',\n",
    "          'PAQ_A-Season', 'PAQ_C-Season', 'SDS-Season', 'PreInt_EduHx-Season']\n",
    "\n",
    "SEED = 42\n",
    "n_splits = 5  # Statified K-Fold 的 fold 數\n",
    "LGBM_Params = {\n",
    "    \"learning_rate\": 0.046,\n",
    "    \"max_depth\": 12,\n",
    "    \"num_leaves\": 478,\n",
    "    \"min_data_in_leaf\": 13,\n",
    "    \"feature_fraction\": 0.893,\n",
    "    \"bagging_fraction\": 0.784,\n",
    "    \"bagging_freq\": 4,\n",
    "    \"lambda_l1\": 10,  # Increased from 6.59\n",
    "    \"lambda_l2\": 0.01,  # Increased from 2.68e-06\n",
    "    # 'device': 'gpu'\n",
    "}\n",
    "XGB_Params = {\n",
    "    \"learning_rate\": 0.05,\n",
    "    \"max_depth\": 6,\n",
    "    \"n_estimators\": 200,\n",
    "    \"subsample\": 0.8,\n",
    "    \"colsample_bytree\": 0.8,\n",
    "    \"reg_alpha\": 1,  # Increased from 0.1\n",
    "    \"reg_lambda\": 5,  # Increased from 1\n",
    "    \"random_state\": SEED,\n",
    "    # 'tree_method': 'gpu_hist',\n",
    "}\n",
    "CatBoost_Params_original = {\n",
    "    \"learning_rate\": 0.05,\n",
    "    \"depth\": 6,\n",
    "    \"iterations\": 200,\n",
    "    \"random_seed\": SEED,\n",
    "    \"verbose\": 0,\n",
    "    \"l2_leaf_reg\": 10,  # Increase this value\n",
    "    # 'task_type': 'GPU'\n",
    "}\n",
    "if EXEC_ENV != EXEC_ENV_ENUM.LOCAL:  # Enable GPU\n",
    "    LGBM_Params['device'] = 'gpu'\n",
    "    XGB_Params['tree_method'] = 'gpu_hist'\n",
    "    CatBoost_Params_original['task_type'] = 'GPU'\n",
    "\n",
    "\"\"\"\n",
    "計算 input_path\n",
    "\"\"\"\n",
    "input_path = \"\"\n",
    "if EXEC_ENV == EXEC_ENV_ENUM.COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount(\"/content/gdrive\")\n",
    "    input_path = (\n",
    "        \"/content/gdrive/MyDrive/DM/child-mind-institute-problematic-internet-use\"\n",
    "    )\n",
    "elif EXEC_ENV == EXEC_ENV_ENUM.KAGGLE:\n",
    "    input_path = \"/kaggle/input/child-mind-institute-problematic-internet-use\"\n",
    "    os.chdir(\"/kaggle/working\")\n",
    "else:\n",
    "    input_path = \"child-mind-institute-problematic-internet-use\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f0941c",
   "metadata": {
    "papermill": {
     "duration": 0.003614,
     "end_time": "2024-12-11T13:33:48.742212",
     "exception": false,
     "start_time": "2024-12-11T13:33:48.738598",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Class & Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "184a6b9f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T13:33:48.751862Z",
     "iopub.status.busy": "2024-12-11T13:33:48.751193Z",
     "iopub.status.idle": "2024-12-11T13:33:48.800048Z",
     "shell.execute_reply": "2024-12-11T13:33:48.799386Z"
    },
    "papermill": {
     "duration": 0.055758,
     "end_time": "2024-12-11T13:33:48.801585",
     "exception": false,
     "start_time": "2024-12-11T13:33:48.745827",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Preprocessing:\n",
    "    def __init__(self, train_csv_input_path,\n",
    "                 test_csv_input_path, \n",
    "                 train_actigraphy_input_path, \n",
    "                 test_actigraphy_input_path):\n",
    "        self.__train = pd.read_csv(train_csv_input_path)\n",
    "        self.__test = pd.read_csv(test_csv_input_path)\n",
    "        self.__train_ts = self.__load_time_series(train_actigraphy_input_path)\n",
    "        self.__test_ts = self.__load_time_series(test_actigraphy_input_path)\n",
    "        # self.__train_act_data = self.__process_pca_on_parquet(train_actigraphy_input_path)\n",
    "        # self.__test_act_data = self.__process_pca_on_parquet(test_actigraphy_input_path)\n",
    "        \n",
    "        # 儲存 id\n",
    "        self.train_id = self.__train[\"id\"]\n",
    "        self.test_id = self.__test[\"id\"]\n",
    "        self.train_ts_id = self.__train_ts[\"id\"]\n",
    "        self.test_ts_id = self.__test_ts[\"id\"]\n",
    "\n",
    "    def __load_time_series(self, dirname) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        回傳 actigraphy 的 dataframe (col: 依序所有 actigraphy column 的 count/mean/std/min/23%/50%/75%/max)，並有一個 col \"id\"\n",
    "        \"\"\"\n",
    "\n",
    "        def process_file(filename, dirname):\n",
    "            \"\"\"\n",
    "            回傳 (actigraphy 依序所有 column 的 count/mean/std/min/23%/50%/75%/max 的一維陣列, id)\n",
    "            \"\"\"\n",
    "\n",
    "            df = pd.read_parquet(os.path.join(dirname, filename, \"part-0.parquet\"))\n",
    "            df.drop(\"step\", axis=1, inplace=True)\n",
    "            return df.describe().values.reshape(-1), filename.split(\"=\")[1]\n",
    "\n",
    "        ids = os.listdir(dirname)\n",
    "\n",
    "        with ThreadPoolExecutor() as executor:\n",
    "            results = list(\n",
    "                tqdm(\n",
    "                    executor.map(lambda fname: process_file(fname, dirname), ids),\n",
    "                    total=len(ids),\n",
    "                )\n",
    "            )\n",
    "\n",
    "        stats, indexes = zip(*results)\n",
    "\n",
    "        df = pd.DataFrame(stats, columns=[f\"stat_{i}\" for i in range(len(stats[0]))])\n",
    "        df[\"id\"] = indexes\n",
    "        return df\n",
    "    \n",
    "    def __change_season_col_dtype_to_category(self, df, cat_c):\n",
    "        for c in cat_c:\n",
    "            df[c] = df[c].fillna('Missing')  # 把 NaN (missing value) 改成字串 \"Missing\"\n",
    "            df[c] = df[c].astype('category')  # 把這些 column 的 dtype 從 object 改成 category (但型別還是 string) (改不改應該對結果沒什麼影響)\n",
    "        return df\n",
    "    def __process_pca_on_parquet(self, dir_path, n_components=5):\n",
    "        # 取得 parquet 文件的列表\n",
    "        train_par = os.listdir(dir_path)\n",
    "    \n",
    "        # 生成新的欄位名稱 (PCA_1, PCA_2, ..., PCA_n_components)\n",
    "        new_columns = [f'PCA_{i}' for i in range(1, n_components + 1)]\n",
    "        new_columns = ['id'] + new_columns\n",
    "    \n",
    "        # 創建最終的 DataFrame，用於保存每個文件處理後的結果\n",
    "        master_df = pd.DataFrame(columns=new_columns)\n",
    "    \n",
    "        # 定義檔案目錄\n",
    "        filedir = dir_path if dir_path.endswith('/') else dir_path + '/'\n",
    "    \n",
    "        # 遍歷每一個 parquet 文件資料夾\n",
    "        for _dir in train_par:\n",
    "            file_name = filedir + _dir + '/part-0.parquet'\n",
    "            # 取得 id (檔案名稱中 split 出來的值)\n",
    "            id = _dir.split('=')[1]\n",
    "    \n",
    "            # 使用 polars 讀取 parquet 文件\n",
    "            act_data = pl.read_parquet(file_name)\n",
    "    \n",
    "            # 標準化數據\n",
    "            scaler = StandardScaler()\n",
    "            X_scaled = scaler.fit_transform(act_data)\n",
    "    \n",
    "            # 應用 PCA，將數據壓縮到指定維度\n",
    "            pca = PCA(n_components=n_components)\n",
    "            X_pca = pca.fit_transform(X_scaled)\n",
    "    \n",
    "            # 將壓縮後的數據轉換成 1x10 向量 (取平均)\n",
    "            pca_vector = X_pca.mean(axis=0).reshape(1, -1)\n",
    "            pca_vector = pca_vector.flatten().astype(object)\n",
    "    \n",
    "            # 插入 id 到向量的第一個位置\n",
    "            flattened = np.insert(pca_vector, 0, id)\n",
    "    \n",
    "            # 創建一個臨時 DataFrame 保存當前結果\n",
    "            temp_df = pd.DataFrame([flattened], columns=new_columns)\n",
    "    \n",
    "            # 將臨時 DataFrame 合併到最終的 master_df\n",
    "            master_df = pd.concat([master_df, temp_df], ignore_index=True)\n",
    "    \n",
    "        # 返回最終的處理後 DataFrame\n",
    "        return master_df\n",
    "\n",
    "    def type1(self):\n",
    "        \"\"\"Submission 1\"\"\"\n",
    "        \n",
    "        def feature_engineering(df):\n",
    "            season_cols = [col for col in df.columns if \"Season\" in col]\n",
    "            df = df.drop(season_cols, axis=1)\n",
    "            df[\"BMI_Age\"] = df[\"Physical-BMI\"] * df[\"Basic_Demos-Age\"]\n",
    "            df[\"Internet_Hours_Age\"] = (\n",
    "                df[\"PreInt_EduHx-computerinternet_hoursday\"] * df[\"Basic_Demos-Age\"]\n",
    "            )\n",
    "            df[\"BMI_Internet_Hours\"] = (\n",
    "                df[\"Physical-BMI\"] * df[\"PreInt_EduHx-computerinternet_hoursday\"]\n",
    "            )\n",
    "            df[\"BFP_BMI\"] = df[\"BIA-BIA_Fat\"] / df[\"BIA-BIA_BMI\"]\n",
    "            df[\"FFMI_BFP\"] = df[\"BIA-BIA_FFMI\"] / df[\"BIA-BIA_Fat\"]\n",
    "            df[\"FMI_BFP\"] = df[\"BIA-BIA_FMI\"] / df[\"BIA-BIA_Fat\"]\n",
    "            df[\"LST_TBW\"] = df[\"BIA-BIA_LST\"] / df[\"BIA-BIA_TBW\"]\n",
    "            df[\"BFP_BMR\"] = df[\"BIA-BIA_Fat\"] * df[\"BIA-BIA_BMR\"]\n",
    "            df[\"BFP_DEE\"] = df[\"BIA-BIA_Fat\"] * df[\"BIA-BIA_DEE\"]\n",
    "            df[\"BMR_Weight\"] = df[\"BIA-BIA_BMR\"] / df[\"Physical-Weight\"]\n",
    "            df[\"DEE_Weight\"] = df[\"BIA-BIA_DEE\"] / df[\"Physical-Weight\"]\n",
    "            df[\"SMM_Height\"] = df[\"BIA-BIA_SMM\"] / df[\"Physical-Height\"]\n",
    "            df[\"Muscle_to_Fat\"] = df[\"BIA-BIA_SMM\"] / df[\"BIA-BIA_FMI\"]\n",
    "            df[\"Hydration_Status\"] = df[\"BIA-BIA_TBW\"] / df[\"Physical-Weight\"]\n",
    "            df[\"ICW_TBW\"] = df[\"BIA-BIA_ICW\"] / df[\"BIA-BIA_TBW\"]\n",
    "\n",
    "            return df\n",
    "\n",
    "        def revise_word_data(df):\n",
    "            season_columns = df.filter(like=\"Seas\").columns\n",
    "            # 定義季節的映射關係\n",
    "            season_mapping = {\n",
    "                \"Spring\": 1,\n",
    "                \"Summer\": 2,\n",
    "                \"Fall\": 3,\n",
    "                \"Winter\": 4\n",
    "            }\n",
    "\n",
    "            # 將季節欄位轉換為數值\n",
    "            for col in season_columns:\n",
    "                df[col] = df[col].map(season_mapping)\n",
    "\n",
    "            # 返回處理後的 DataFrame\n",
    "            return df            \n",
    "\n",
    "        def merge_file(org, act):\n",
    "            file1 = org\n",
    "            file2 = act\n",
    "            merged_file = pd.merge(file1, file2, on='id', how = 'left')\n",
    "\n",
    "            return merged_file\n",
    "        \n",
    "        def impute(dataframe):\n",
    "            df = dataframe\n",
    "\n",
    "            df_numeric = df.select_dtypes(include=['float64', 'int64'])  # 只選擇數值型欄位\n",
    "\n",
    "            # 使用 SimpleImputer 進行填補\n",
    "            imputer = KNNImputer(n_neighbors=5)  # 可以調整 n_neighbors 來設定最近鄰居的數量\n",
    "            df_numeric_imputed = pd.DataFrame(imputer.fit_transform(df_numeric), columns=df_numeric.columns)\n",
    "            df_numeric_imputed[\"sii\"] = df_numeric_imputed[\"sii\"].round().astype(int)\n",
    "\n",
    "            # 替換原 DataFrame 中的數值型欄位\n",
    "            df[df_numeric.columns] = df_numeric_imputed\n",
    "\n",
    "            # 返回處理後的 DataFrame\n",
    "            return df\n",
    "\n",
    "        def drop_features(train, test):\n",
    "            features_not_in_test = []\n",
    "            for f in train.columns:\n",
    "                if f not in test.columns:\n",
    "                    features_not_in_test.append(f)\n",
    "\n",
    "            print(features_not_in_test)\n",
    "\n",
    "            features_not_in_test.pop()\n",
    "            features_not_in_test.pop(0)\n",
    "            train_revised = train.drop(columns=features_not_in_test)\n",
    "\n",
    "            return train_revised\n",
    "\n",
    "        def perform_autoencoder(df, encoding_dim=50, epochs=50, batch_size=32):\n",
    "            # 將 dataframe 轉成 z-score\n",
    "            scaler = StandardScaler()\n",
    "            df_scaled = scaler.fit_transform(df)\n",
    "\n",
    "            data_tensor = torch.FloatTensor(df_scaled)\n",
    "\n",
    "            input_dim = data_tensor.shape[1]\n",
    "            autoencoder = AutoEncoder(input_dim, encoding_dim)\n",
    "\n",
    "            criterion = nn.MSELoss()\n",
    "            optimizer = optim.Adam(autoencoder.parameters())\n",
    "\n",
    "            for epoch in range(epochs):\n",
    "                for i in range(0, len(data_tensor), batch_size):\n",
    "                    batch = data_tensor[i : i + batch_size]\n",
    "                    optimizer.zero_grad()\n",
    "                    reconstructed = autoencoder(batch)\n",
    "                    loss = criterion(reconstructed, batch)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                if (epoch + 1) % 10 == 0:\n",
    "                    print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}]\")\n",
    "\n",
    "            with torch.no_grad():\n",
    "                encoded_data = autoencoder.encoder(data_tensor).numpy()\n",
    "\n",
    "            df_encoded = pd.DataFrame(\n",
    "                encoded_data, columns=[f\"Enc_{i + 1}\" for i in range(encoded_data.shape[1])]\n",
    "            )\n",
    "\n",
    "            return df_encoded\n",
    "        def perform_pca(df, n_components):\n",
    "            \"\"\"\n",
    "            Perform PCA on the given DataFrame and return the reduced DataFrame.\n",
    "\n",
    "            Parameters:\n",
    "            - df: DataFrame to process\n",
    "            - n_components: Number of principal components to keep\n",
    "\n",
    "            Returns:\n",
    "            - A DataFrame with reduced dimensions\n",
    "            \"\"\"\n",
    "            # Z-score normalization\n",
    "            scaler = StandardScaler()\n",
    "            df_scaled = scaler.fit_transform(df)\n",
    "\n",
    "            # Apply PCA\n",
    "            pca = PCA(n_components=n_components)\n",
    "            reduced_data = pca.fit_transform(df_scaled)\n",
    "\n",
    "            # Convert to DataFrame\n",
    "            df_reduced = pd.DataFrame(\n",
    "                reduced_data, columns=[f\"PCA_{i + 1}\" for i in range(n_components)]\n",
    "            )\n",
    "            return df_reduced\n",
    "        \n",
    "        def perform_anova(df, df_test):\n",
    "            labels = df['sii']\n",
    "            df = df.drop(['sii'], axis=1)\n",
    "\n",
    "            X = df.iloc[:, :-1]\n",
    "            y = df.iloc[:, -1]\n",
    "\n",
    "            imputer = KNNImputer(n_neighbors=5)\n",
    "            X_imputed = pd.DataFrame(imputer.fit_transform(X), columns=X.columns)  \n",
    "\n",
    "            selector = SelectKBest(score_func=f_classif, k=20)\n",
    "            selector.fit(X_imputed, y)\n",
    "\n",
    "            selected_features = X.columns[selector.get_support()]\n",
    "\n",
    "            new_df = df[selected_features]\n",
    "            df_test = df_test[selected_features]\n",
    "            new_df['sii'] = labels\n",
    "\n",
    "            return new_df, df_test\n",
    "        \n",
    "        def perform_rf(df, df_test):\n",
    "            labels = df['sii']\n",
    "            df = df.drop(['sii'], axis=1)\n",
    "\n",
    "            # 特徵矩陣與標籤\n",
    "            X = df\n",
    "            y = labels\n",
    "\n",
    "            # 缺失值補全\n",
    "            imputer = KNNImputer(n_neighbors=5)\n",
    "            X_imputed = pd.DataFrame(imputer.fit_transform(X), columns=X.columns)\n",
    "\n",
    "            # 使用 Random Forest 訓練模型\n",
    "            rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "            rf.fit(X_imputed, y)\n",
    "\n",
    "            # 根據特徵重要性選擇前 20 個特徵\n",
    "            feature_importances = rf.feature_importances_\n",
    "            important_indices = feature_importances.argsort()[-20:][::-1]\n",
    "            selected_features = X.columns[important_indices]\n",
    "\n",
    "            # 篩選新特徵\n",
    "            new_df = X_imputed[selected_features]\n",
    "            df_test = df_test[selected_features]\n",
    "            new_df['sii'] = labels\n",
    "\n",
    "            return new_df, df_test\n",
    "        \n",
    "        def perform_lasso(df, df_test):\n",
    "            labels = df['sii']\n",
    "            df = df.drop(['sii'], axis=1)\n",
    "\n",
    "            # 特徵矩陣與標籤\n",
    "            X = df\n",
    "            y = labels\n",
    "\n",
    "            # 缺失值補全\n",
    "            imputer = KNNImputer(n_neighbors=5)\n",
    "            X_imputed = pd.DataFrame(imputer.fit_transform(X), columns=X.columns)\n",
    "\n",
    "            # 標準化數據\n",
    "            scaler = StandardScaler()\n",
    "            X_scaled = scaler.fit_transform(X_imputed)\n",
    "\n",
    "            # 使用 Lasso 訓練模型\n",
    "            lasso = Lasso(alpha=0.01, random_state=42)  # 可以根據需求調整 alpha\n",
    "            lasso.fit(X_scaled, y)\n",
    "\n",
    "            # 選擇非零權重的特徵\n",
    "            selected_features = X.columns[lasso.coef_ != 0]\n",
    "\n",
    "            # 篩選新特徵\n",
    "            new_df = X_imputed[selected_features]\n",
    "            df_test = df_test[selected_features]\n",
    "            new_df['sii'] = labels\n",
    "\n",
    "            return new_df, df_test            \n",
    "\n",
    "        train = copy.deepcopy(self.__train)\n",
    "        test = copy.deepcopy(self.__test)\n",
    "\n",
    "        # Load in actigraphy data\n",
    "        train_ts = copy.deepcopy(self.__train_ts)\n",
    "        test_ts = copy.deepcopy(self.__test_ts)\n",
    "\n",
    "        train_ts = train_ts.drop(\"id\", axis=1)\n",
    "        test_ts = test_ts.drop(\"id\", axis=1)\n",
    "\n",
    "        # 對 actigraphy data 用 autoencoder 做 encode\n",
    "        train_ts = perform_autoencoder(\n",
    "            train_ts, encoding_dim=60, epochs=100, batch_size=32\n",
    "        )\n",
    "        test_ts = perform_autoencoder(\n",
    "            test_ts, encoding_dim=60, epochs=100, batch_size=32\n",
    "        )\n",
    "\n",
    "        # train_ts = perform_pca(train_ts, n_components=2)\n",
    "        # test_ts = perform_pca(test_ts, n_components=2)\n",
    "\n",
    "\n",
    "        encode_cols_name = train_ts.columns.tolist()  # Enc_1 ~ Enc_<encoding_dim>\n",
    "\n",
    "        train_ts[\"id\"] = self.train_ts_id\n",
    "        test_ts[\"id\"] = self.test_ts_id\n",
    "\n",
    "        # Merge original data and actigraphy\n",
    "        merged_train = merge_file(train, train_ts)\n",
    "        merged_test = merge_file(test, test_ts)\n",
    "\n",
    "        # Use KNN imputer to impute training dataset\n",
    "        train_imputed = impute(merged_train)\n",
    "\n",
    "        # Revise word data (Season)\n",
    "        train_imputed = revise_word_data(train_imputed)\n",
    "        merged_test = revise_word_data(merged_test)\n",
    "\n",
    "        train_imputed = drop_features(train_imputed, merged_test)\n",
    "\n",
    "        # Seperate into two training dataset (有做測驗 & 沒做測驗)\n",
    "        train_PCIAT = train_imputed[train_imputed['PCIAT-Season'].notna()]\n",
    "        train_No_PCIAT = train_imputed[train_imputed['PCIAT-Season'].isna()]\n",
    "\n",
    "        train_PCIAT = train_PCIAT.drop(\"PCIAT-Season\", axis=1)\n",
    "        train_No_PCIAT = train_No_PCIAT.drop(\"PCIAT-Season\", axis=1)\n",
    "\n",
    "        # Feature engineering\n",
    "        train_PCIAT = feature_engineering(train_PCIAT)\n",
    "        train_No_PCIAT = feature_engineering(train_No_PCIAT)\n",
    "        merged_test = feature_engineering(merged_test)\n",
    "        train_imputed = feature_engineering(train_imputed)\n",
    "\n",
    "        # 移除 id\n",
    "        train_PCIAT = train_PCIAT.drop(\"id\", axis=1)\n",
    "        train_NO_PCIAT = train_No_PCIAT.drop(\"id\", axis=1)\n",
    "        merged_test = merged_test.drop(\"id\", axis=1)\n",
    "\n",
    "        train_imputed = train_imputed.drop(\"id\", axis=1)\n",
    "\n",
    "        if np.any(np.isinf(train_PCIAT)):\n",
    "            train_PCIAT = train_PCIAT.replace([np.inf, -np.inf], np.nan)\n",
    "        if np.any(np.isinf(train_NO_PCIAT)):\n",
    "            train_PCIAT = train_NO_PCIAT.replace([np.inf, -np.inf], np.nan)\n",
    "        if np.any(np.isinf(train_imputed)):\n",
    "            train_imputed = train_imputed.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "        # train_anova, merged_test = perform_anova(train_imputed, merged_test)\n",
    "        train_anova, merged_test = perform_lasso(train_imputed, merged_test)\n",
    "        # train_anova, merged_test = perform_rf(train_imputed, merged_test)\n",
    "\n",
    "        # return train_PCIAT, train_No_PCIAT, merged_test\n",
    "        return train_anova, merged_test\n",
    "    \n",
    "    def type2(self):\n",
    "        \"\"\"Submission 2 and 3 (portion)\"\"\"\n",
    "\n",
    "        def create_mapping(column, dataset):\n",
    "            '''\n",
    "            回傳 column 內的 unique value dictionary (key: 值、value: 對應的一數字)\n",
    "            '''\n",
    "\n",
    "            unique_values = dataset[column].unique()\n",
    "            return {value: idx for idx, value in enumerate(unique_values)}\n",
    "\n",
    "        train = copy.deepcopy(self.__train)\n",
    "        test = copy.deepcopy(self.__test)\n",
    "        train_ts = copy.deepcopy(self.__train_ts)\n",
    "        test_ts = copy.deepcopy(self.__test_ts)\n",
    "\n",
    "        # 儲存處理好 (但不做 Encode) 的 column names\n",
    "        time_series_cols_name = train_ts.columns.tolist()\n",
    "        time_series_cols_name.remove(\"id\")\n",
    "\n",
    "        # merge csv 資料跟處理好的 actigraphy data\n",
    "        train = pd.merge(train, train_ts, how=\"left\", on='id')\n",
    "        test = pd.merge(test, test_ts, how=\"left\", on='id')\n",
    "\n",
    "        # 移除 id\n",
    "        train = train.drop(\"id\", axis=1)\n",
    "        test = test.drop(\"id\", axis=1)\n",
    "\n",
    "        # 只取某些 column\n",
    "        featuresCols = copy.deepcopy(train_featuresCols[1]) + time_series_cols_name\n",
    "        train = train[featuresCols].dropna(subset='sii')\n",
    "\n",
    "        train = self.__change_season_col_dtype_to_category(train, season_cols)\n",
    "        test = self.__change_season_col_dtype_to_category(test, season_cols)\n",
    "\n",
    "        # 將所有 season columns 的值變為 numeric\n",
    "        for col in season_cols:\n",
    "            mapping_train = create_mapping(col, train)\n",
    "            mapping_test = create_mapping(col, test)\n",
    "\n",
    "            train[col] = train[col].replace(mapping_train).astype(int)\n",
    "            test[col] = test[col].replace(mapping_test).astype(int)\n",
    "\n",
    "        return train, test\n",
    "\n",
    "\n",
    "class AutoEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, encoding_dim):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, encoding_dim * 3),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(encoding_dim * 3, encoding_dim * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(encoding_dim * 2, encoding_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(encoding_dim, input_dim * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(input_dim * 2, input_dim * 3),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(input_dim * 3, input_dim),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "def quadratic_weighted_kappa(y_true, y_pred):\n",
    "    return cohen_kappa_score(y_true, y_pred, weights=\"quadratic\")\n",
    "\n",
    "\n",
    "def TrainML(model_class, train_data, test_data):\n",
    "\n",
    "    def threshold_Rounder(oof_non_rounded, thresholds):\n",
    "        \"\"\"\n",
    "        1. 若 oof_non_rounded 中的值 < thresholds[0]，則將該值標記為 0。\n",
    "        2. 若 oof_non_rounded 的值在 thresholds[0] 和 thresholds[1] 之間，則標記為 1。\n",
    "        3. 若 oof_non_rounded 的值在 thresholds[1] 和 thresholds[2] 之間，則標記為 2。\n",
    "        4. 若 oof_non_rounded 的值 >= thresholds[2]，則標記為 3。\n",
    "        \"\"\"\n",
    "\n",
    "        return np.where(\n",
    "            oof_non_rounded < thresholds[0],\n",
    "            0,\n",
    "            np.where(\n",
    "                oof_non_rounded < thresholds[1],\n",
    "                1,\n",
    "                np.where(oof_non_rounded < thresholds[2], 2, 3),\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def evaluate_predictions(thresholds, y_true, oof_non_rounded):\n",
    "        rounded_p = threshold_Rounder(oof_non_rounded, thresholds)\n",
    "        return -quadratic_weighted_kappa(y_true, rounded_p)\n",
    "\n",
    "    X = train_data.drop([\"sii\"], axis=1)\n",
    "    y = train_data[\"sii\"]\n",
    "\n",
    "    \"\"\"\n",
    "    Statified K-Fold: K-Fold 並確保訓練集，測試集中各類別樣本的比例與原始資料集中相同\n",
    "\n",
    "    n_splits: fold 數\n",
    "    \"\"\"\n",
    "    SKF = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n",
    "    # KF = KFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n",
    "\n",
    "    train_kappas = []\n",
    "    test_kappas = []\n",
    "\n",
    "    oof_non_rounded = np.zeros(len(y), dtype=float)\n",
    "    # oof_rounded = np.zeros(len(y), dtype=int)\n",
    "    test_preds = np.zeros((len(test_data), n_splits))\n",
    "\n",
    "    for fold, (train_indices, val_indices) in enumerate(\n",
    "        tqdm(SKF.split(X, y), desc=\"Training Folds\", total=n_splits)\n",
    "    ):\n",
    "        X_train, X_val = X.iloc[train_indices], X.iloc[val_indices]\n",
    "        y_train, y_val = y.iloc[train_indices], y.iloc[val_indices]\n",
    "\n",
    "        model = clone(model_class)\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        y_train_pred = model.predict(X_train)\n",
    "        y_val_pred = model.predict(X_val)\n",
    "\n",
    "        oof_non_rounded[val_indices] = y_val_pred\n",
    "        y_val_pred_rounded = y_val_pred.round(0).astype(int)\n",
    "        # oof_rounded[val_indices] = y_val_pred_rounded\n",
    "\n",
    "        train_kappa = quadratic_weighted_kappa(\n",
    "            y_train, y_train_pred.round(0).astype(int)\n",
    "        )\n",
    "        val_kappa = quadratic_weighted_kappa(y_val, y_val_pred_rounded)\n",
    "\n",
    "        train_kappas.append(train_kappa)\n",
    "        test_kappas.append(val_kappa)\n",
    "\n",
    "        test_preds[:, fold] = model.predict(test_data)\n",
    "\n",
    "        print(\n",
    "            f\"Fold {fold+1} - Train QWK: {train_kappa:.4f}, Validation QWK: {val_kappa:.4f}\"\n",
    "        )\n",
    "        clear_output(wait=True)\n",
    "\n",
    "    print(f\"Mean Train QWK --> {np.mean(train_kappas):.4f}\")\n",
    "    print(f\"Mean Validation QWK ---> {np.mean(test_kappas):.4f}\")\n",
    "\n",
    "    \"\"\"\n",
    "    找到預測出來的浮點數 sii 若要轉為整數，要用 threshold 多少才能讓效果最好\n",
    "    原始 threshold 為 x0，optimized 後的 threshold 為 KappaOptimizer.x\n",
    "    \"\"\"\n",
    "    KappaOptimizer = minimize(\n",
    "        evaluate_predictions,\n",
    "        x0=[0.5, 1.5, 2.5],\n",
    "        args=(y, oof_non_rounded),\n",
    "        method=\"Nelder-Mead\",\n",
    "    )\n",
    "    assert KappaOptimizer.success, \"Optimization did not converge.\"\n",
    "\n",
    "    oof_optimized_rounded = threshold_Rounder(oof_non_rounded, KappaOptimizer.x)\n",
    "    tKappa = quadratic_weighted_kappa(y, oof_optimized_rounded)\n",
    "\n",
    "    print(\n",
    "        f\"----> || Optimized QWK SCORE :: {Fore.CYAN}{Style.BRIGHT} {tKappa:.3f}{Style.RESET_ALL}\"\n",
    "    )\n",
    "\n",
    "    \"\"\"\n",
    "    cross-validation 的各 fold 預測的值取 mean\n",
    "    \"\"\"\n",
    "    tpm = test_preds.mean(axis=1)\n",
    "    tpTuned = threshold_Rounder(tpm, KappaOptimizer.x)\n",
    "\n",
    "    sample = pd.read_csv(os.path.join(input_path, \"sample_submission.csv\"))\n",
    "    submission = pd.DataFrame({\"id\": sample[\"id\"], \"sii\": tpTuned})\n",
    "\n",
    "    return submission"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1b9e17",
   "metadata": {
    "papermill": {
     "duration": 0.003413,
     "end_time": "2024-12-11T13:33:48.808644",
     "exception": false,
     "start_time": "2024-12-11T13:33:48.805231",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8fe908b9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T13:33:48.816998Z",
     "iopub.status.busy": "2024-12-11T13:33:48.816749Z",
     "iopub.status.idle": "2024-12-11T13:35:01.252453Z",
     "shell.execute_reply": "2024-12-11T13:35:01.251460Z"
    },
    "papermill": {
     "duration": 72.441767,
     "end_time": "2024-12-11T13:35:01.254162",
     "exception": false,
     "start_time": "2024-12-11T13:33:48.812395",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 996/996 [01:12<00:00, 13.81it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 10.84it/s]\n"
     ]
    }
   ],
   "source": [
    "preprocessing = Preprocessing(\n",
    "    train_csv_input_path=os.path.join(input_path, \"train.csv\"),\n",
    "    train_actigraphy_input_path=os.path.join(input_path, \"series_train.parquet\"),\n",
    "    test_csv_input_path=os.path.join(input_path, \"test.csv\"),\n",
    "    test_actigraphy_input_path=os.path.join(input_path, \"series_test.parquet\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e964f0",
   "metadata": {
    "papermill": {
     "duration": 0.017886,
     "end_time": "2024-12-11T13:35:01.289653",
     "exception": false,
     "start_time": "2024-12-11T13:35:01.271767",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Submission 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2f5dbc2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T13:35:01.323769Z",
     "iopub.status.busy": "2024-12-11T13:35:01.323498Z",
     "iopub.status.idle": "2024-12-11T13:35:50.325328Z",
     "shell.execute_reply": "2024-12-11T13:35:50.324538Z"
    },
    "papermill": {
     "duration": 49.021437,
     "end_time": "2024-12-11T13:35:50.327500",
     "exception": false,
     "start_time": "2024-12-11T13:35:01.306063",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Folds: 100%|██████████| 5/5 [00:23<00:00,  4.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Train QWK --> 0.7671\n",
      "Mean Validation QWK ---> 0.4831\n",
      "----> || Optimized QWK SCORE :: \u001b[36m\u001b[1m 0.535\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# train: With PCIAT, train2: No_PCIAT\n",
    "#train, train2, test = preprocessing.type1()\n",
    "train, test = preprocessing.type1()\n",
    "\n",
    "print(\"Preprocessing_finished\")\n",
    "\n",
    "# Create model instances\n",
    "# FIXME: 小型數據集 LGBM 容易 overfitting\n",
    "Light = LGBMRegressor(\n",
    "    **LGBM_Params, random_state=SEED, verbose=-1, n_estimators=300\n",
    ")\n",
    "XGB_Model = XGBRegressor(**XGB_Params)\n",
    "CatBoost_Model = CatBoostRegressor(**CatBoost_Params_original)\n",
    "\n",
    "# Combine models using Voting Regressor\n",
    "voting_model = VotingRegressor(\n",
    "    estimators=[\n",
    "        (\"lightgbm\", Light),\n",
    "        (\"xgboost\", XGB_Model),\n",
    "        (\"catboost\", CatBoost_Model),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Train the ensemble model\n",
    "Submission1 = TrainML(voting_model, train, test)\n",
    "\n",
    "Submission1.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af07c9e3",
   "metadata": {
    "papermill": {
     "duration": 0.030948,
     "end_time": "2024-12-11T13:35:50.389613",
     "exception": false,
     "start_time": "2024-12-11T13:35:50.358665",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Submission 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39f696ee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T13:35:50.452434Z",
     "iopub.status.busy": "2024-12-11T13:35:50.452085Z",
     "iopub.status.idle": "2024-12-11T13:36:23.192407Z",
     "shell.execute_reply": "2024-12-11T13:36:23.191624Z"
    },
    "papermill": {
     "duration": 32.774129,
     "end_time": "2024-12-11T13:36:23.194299",
     "exception": false,
     "start_time": "2024-12-11T13:35:50.420170",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Folds: 100%|██████████| 5/5 [00:32<00:00,  6.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Train QWK --> 0.7627\n",
      "Mean Validation QWK ---> 0.3961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----> || Optimized QWK SCORE :: \u001b[36m\u001b[1m 0.455\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "train, test = preprocessing.type2()\n",
    "\n",
    "CatBoost_Params = copy.deepcopy(CatBoost_Params_original)\n",
    "CatBoost_Params['cat_features'] = season_cols\n",
    "\n",
    "Light = LGBMRegressor(**LGBM_Params, random_state=SEED, verbose=-1, n_estimators=300)\n",
    "XGB_Model = XGBRegressor(**XGB_Params)\n",
    "CatBoost_Model = CatBoostRegressor(**CatBoost_Params)\n",
    "\n",
    "voting_model = VotingRegressor(estimators=[\n",
    "    ('lightgbm', Light),\n",
    "    ('xgboost', XGB_Model),\n",
    "    ('catboost', CatBoost_Model)\n",
    "])\n",
    "\n",
    "Submission2 = TrainML(voting_model, train, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b43f7d",
   "metadata": {
    "papermill": {
     "duration": 0.016735,
     "end_time": "2024-12-11T13:36:23.228878",
     "exception": false,
     "start_time": "2024-12-11T13:36:23.212143",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Submission 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de0eb575",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T13:36:23.263911Z",
     "iopub.status.busy": "2024-12-11T13:36:23.263629Z",
     "iopub.status.idle": "2024-12-11T13:38:23.679515Z",
     "shell.execute_reply": "2024-12-11T13:38:23.678388Z"
    },
    "papermill": {
     "duration": 120.435801,
     "end_time": "2024-12-11T13:38:23.681319",
     "exception": false,
     "start_time": "2024-12-11T13:36:23.245518",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Folds: 100%|██████████| 5/5 [02:00<00:00, 24.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Train QWK --> 0.9175\n",
      "Mean Validation QWK ---> 0.3803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----> || Optimized QWK SCORE :: \u001b[36m\u001b[1m 0.450\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "train, test = preprocessing.type2()\n",
    "\n",
    "# Submisssion 3 另外的 preprocessing: 填 missing value 方法使用取 median\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "\n",
    "ensemble = VotingRegressor(estimators=[\n",
    "    ('lgb', Pipeline(steps=[('imputer', imputer), ('regressor', LGBMRegressor(random_state=SEED))])),\n",
    "    ('xgb', Pipeline(steps=[('imputer', imputer), ('regressor', XGBRegressor(random_state=SEED))])),\n",
    "    ('cat', Pipeline(steps=[('imputer', imputer), ('regressor', CatBoostRegressor(random_state=SEED, silent=True))])),\n",
    "    ('rf', Pipeline(steps=[('imputer', imputer), ('regressor', RandomForestRegressor(random_state=SEED))])),\n",
    "    ('gb', Pipeline(steps=[('imputer', imputer), ('regressor', GradientBoostingRegressor(random_state=SEED))]))\n",
    "])\n",
    "\n",
    "Submission3 = TrainML(ensemble, train, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f20a592",
   "metadata": {
    "papermill": {
     "duration": 0.017092,
     "end_time": "2024-12-11T13:38:23.715960",
     "exception": false,
     "start_time": "2024-12-11T13:38:23.698868",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Submission 1 + 2 + 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2b00b01",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T13:38:23.751088Z",
     "iopub.status.busy": "2024-12-11T13:38:23.750803Z",
     "iopub.status.idle": "2024-12-11T13:38:23.764366Z",
     "shell.execute_reply": "2024-12-11T13:38:23.763717Z"
    },
    "papermill": {
     "duration": 0.033021,
     "end_time": "2024-12-11T13:38:23.765927",
     "exception": false,
     "start_time": "2024-12-11T13:38:23.732906",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "Submission1 = Submission1.sort_values(by='id').reset_index(drop=True)\n",
    "Submission2 = Submission2.sort_values(by='id').reset_index(drop=True)\n",
    "Submission3 = Submission3.sort_values(by='id').reset_index(drop=True)\n",
    "\n",
    "combined = pd.DataFrame({\n",
    "    'id': Submission1['id'],\n",
    "    'sii_1': Submission1['sii'],\n",
    "    'sii_2': Submission2['sii'],\n",
    "    'sii_3': Submission3['sii']\n",
    "})\n",
    "\n",
    "def majority_vote(row):\n",
    "    return row.mode()[0]  # mode: 眾數\n",
    "\n",
    "combined['final_sii'] = combined[['sii_1', 'sii_2', 'sii_3']].apply(majority_vote, axis=1)\n",
    "\n",
    "final_submission = combined[['id', 'final_sii']].rename(columns={'final_sii': 'sii'})\n",
    "\n",
    "# final_submission.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 9643020,
     "sourceId": 81933,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30805,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 340.841453,
   "end_time": "2024-12-11T13:38:27.100515",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-12-11T13:32:46.259062",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
